# åŸºäºæœºå™¨å­¦ä¹ çš„æ¶æ„URLæ£€æµ‹

## æ¦‚è¿°

æœ¬æ¬¡å®è·µè™½é¢˜ä¸ºâ€åŸºäºæœºå™¨å­¦ä¹ â€œï¼Œä½†ç›®å‰ä¹Ÿåªæ›´æ–°åˆ°ä½¿ç”¨TF-IDFå’Œçº¿æ€§å›å½’æ¨¡å‹çš„è¿›åº¦ã€‚ä¸è¿‡åƒé‡Œä¹‹è¡Œå§‹äºè¶³ä¸‹å˜›ï¼Œåé¢å†æ›´å•¦ğŸ¦

## æå‡ºé—®é¢˜

é¦–å…ˆæˆ‘ä»¬éœ€è¦å°†å®‰å…¨é—®é¢˜è¿›è¡ŒæŠ½è±¡ï¼Œä¹Ÿå°±æ˜¯é’ˆå¯¹ç°çŠ¶æå‡ºé—®é¢˜ã€‚åœ¨å‡å®šçš„è¿™ä¸ªä¸šåŠ¡èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å‘ç°ï¼š

-   æ¶æ„URLå­˜åœ¨ç‰¹å®šå‡ ç§ç±»å‹1

-   ç‰¹å®šç±»å‹æ¶æ„URLåœ¨æ–‡æœ¬ä¸Šå­˜åœ¨æ™®éçš„**è¯æ±‡ç‰¹å¾**ï¼Œä¾‹å¦‚é’“é±¼URLä¸­å¸¸è§"login", "account", "sigin"ç­‰å…³é”®è¯

å› æ­¤æˆ‘ä»¬å°è¯•ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•å¯¹æ¶æ„URLè¿›è¡Œæ£€æµ‹åˆ†æã€‚



## æ•°æ®å¤„ç†

### æ ·æœ¬é€‰æ‹©

-   [malicious-URLs](https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs)
    -   **malicious-URLs** åœ¨Githubä¸Šé¢ä¸€ä¸ª ä½¿ç”¨æœºå™¨å­¦ä¹ å»æ£€æµ‹æ¶æ„URLçš„é¡¹ç›® ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªè®­ç»ƒé›†ï¼Œæœ‰åšæ ‡è®°æ˜¯æ­£å¸¸çš„URLè¿˜æ˜¯æ¶æ„çš„URL
    -   å†…å®¹ç±»å‹ï¼šæ–‡æœ¬æ ·æœ¬
    -   æ˜¯å¦æ ‡è®°ï¼šæ˜¯
    -   æ˜¯å¦ç‰¹å¾åŒ–ï¼šå¦
    -   ä½¿ç”¨èŒƒå›´ï¼šå…¥ä¾µæ£€æµ‹ã€å¼‚å¸¸æµé‡ã€WAF

### æ•°æ®æ¸…æ´—

ç”±äºæ ·æœ¬æœ¬èº«ä¸ºå¤„ç†å¥½çš„æ ‡è®°æ•°æ®ï¼Œæ‰€ä»¥åœ¨æ•°æ®æ ¼å¼å’Œè„æ•°æ®ä¸Šæ— éœ€å¤„ç†ï¼ˆçœŸå®æƒ…å†µä¸‹å¯èƒ½æ­£å¥½ç›¸å:-(ï¼‰ã€‚

ç¼–å†™æ•°æ®å¸§æå–å‡½æ•°ï¼š

```python
def csv_data_read(csv_file_path):
    # ä¸ºå‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå¯åªå–å¤´éƒ¨10Wæ¡ï¼Œä½†ä¸€å®šéœ€è¦å…ˆæ‰“ä¹±æ ·æœ¬ï¼‰
    # df_csv = pd.read_csv(csv_file_path).head(100000)
    df_csv = pd.read_csv(csv_file_path)
    urls = []
    labels = []
    for index, row in df_csv.iterrows():
        urls.append(row["url"])
        labels.append(row["label"])
    return urls, labels
```

ç¼–å†™å¯¹URLçš„æ•°æ®æ¸…æ´—å‡½æ•°ï¼š
```python
def url_tokenize(url):
    """
    å¯¹URLè¿›è¡Œæ¸…æ´—ï¼Œåˆ é™¤æ–œçº¿ã€ç‚¹ã€å’Œcomï¼Œè¿›è¡Œåˆ†è¯
    :param url:
    :return:
    """
    web_url = url.lower()
    dot_slash = []
    slash = str(web_url).split('/')
    for i in slash:
        r1 = str(i).split('-')
        token_slash = []
        for j in range(0,len(r1)):
            r2 = str(r1[j]).split('.')
            token_slash = token_slash + r2
        dot_slash = dot_slash + r1 + token_slash
    urltoken_list = list(set(dot_slash))
    white_words = ["com", "http:", "https:", ""]
    for white_word in white_words:
        if white_word in urltoken_list:
            urltoken_list.remove(white_word)
    return urltoken_list
```



## ç‰¹å¾æå–

æˆ‘ä»¬é¦–å…ˆåŠ è½½æ•°æ®é›†ï¼š

```python
    grep_csv_file_path = "../../data/data-0x3/grey-url.csv"
    black_csv_file_path = "../../data/data-0x3/black-url.csv"
    grey_urls, y = csv_data_read(grep_csv_file_path)
```

æˆ‘ä»¬ä½¿ç”¨TF-IDFç®—æ³•æå–URLçš„ç‰¹å¾ï¼Œå¹¶å°†æ•°æ®å¸§åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```python
    url_vectorizer = TfidfVectorizer(tokenizer=url_tokenize)
    x = url_vectorizer.fit_transform(grey_urls)
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
```

### æ³¨ï¼šTF-IDF

TF-IDF(Term Frequency â€“ Inverse Document Frequency)ï¼Œå³è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ã€‚åœ¨è®¡ç®—ä¸Šä¸ºè¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡çš„ä¹˜ç§¯ã€‚è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š

-   è®¡ç®—è¯é¢‘ï¼ˆTFï¼‰
    -   æŸä¸ªè¯åœ¨æ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°/æ–‡ç« çš„æ€»è¯æ•°
    -   å³æŸä¸ªè¯åœ¨è¿™æ®µæ–‡å­—ä¸­å‡ºç°å¾—è¶Šå¤šï¼ŒTFå°±è¶Šå¤§
-   è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰
    -   log(è¯­æ–™åº“çš„æ–‡æ¡£æ€»æ•°/åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°+1)
    -   æŸä¸ªè¯åœ¨æ™®éæƒ…å†µä¸‹è¶Šå¸¸è§ï¼Œåˆ†æ¯å¤§ï¼ŒIDFä¹Ÿçº¦è¶‹äº0
-   è®¡ç®—TF-IDF
    -   TF-IDF = TF * IDF
    -   TF-IDFè¶Šå¤§ï¼Œè¯´æ˜è¯åœ¨è¿™æ®µæ–‡ç« ä¸­è¶Šé‡è¦ï¼Œä½†å› ä¸ºæœ‰IDFçš„å­˜åœ¨ï¼Œåˆèƒ½é¿å…æŠŠâ€œæ˜¯â€ã€â€çš„â€œã€â€œå’Œâ€ç­‰åœç”¨è¯çš„TF-IDFå€¼é™ä½

åœ¨åº”ç”¨ä¸Šï¼šå°†æ–‡ç« åˆ†è¯ï¼Œè®¡ç®—TF-IDFï¼ŒæŒ‰ç…§å…¶å€¼å¤§å°é™åºæ’åˆ—ï¼Œæ’åé å‰çš„å³æ–‡ç« çš„å…³é”®è¯



## æ¨¡å‹é€‰æ‹©

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†ä½¿ç”¨**é€»è¾‘å›å½’æ¨¡å‹**ï¼Œå¹¶å°†å°†æ‹Ÿåˆåçš„æ¨¡å‹å’Œå‘é‡ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ï¼Œä¾¿äºé‡å¤ä½¿ç”¨

```python
    # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‰§è¡Œé€»è¾‘å›å½’
    l_regress = LogisticRegression(solver='liblinear')
    l_regress.fit(x_train, y_train)
    l_score = l_regress.score(x_test, y_test)
    # print("æµ‹è¯•æ‹Ÿåˆåˆ†æ•°ä¸ºï¼š{0}".format(l_score))

    file_mode = "../../model/model-0x3/model.pkl"
    dump_model_object(file_mode)
    file_vector = "../../model/model-0x3/vector.pl"
    dump_model_object(file_vector)
```





## æ•ˆæœè¯„ä¼°

åœ¨â€œç‰¹å¾æå–â€éƒ¨åˆ†æˆ‘ä»¬é‡‡ç”¨`train_test_split`æ–¹æ³•è¿›è¡Œéšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè¿›è¡Œ**äº¤å‰éªŒè¯**ï¼Œå†æ¬¡å›é¡¾ä»£ç ä¸ºï¼š

```python
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
```

ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œæœ€åå¾—åˆ°æµ‹è¯•æ‹Ÿåˆåˆ†æ•°ä¸ºï¼š**0.9599966703530615**

æ³¨ï¼Œå‚æ•°ä»‹ç»ï¼š

-   `test_size`ï¼šæµ‹è¯•é›†åœ¨æ€»æ ·æœ¬ä¸­çš„å æ¯”
-   `random_state`ï¼šéšæœºæ•°çš„ç§å­ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºè¯¥ç»„éšæœºæ•°çš„ç¼–å·ã€‚è§„åˆ™æ˜¯ï¼šç§å­ä¸åŒæ—¶ï¼Œäº§ç”Ÿä¸åŒçš„éšæœºæ•°ï¼›ç§å­ç›¸åŒæ—¶ï¼Œåœ¨ä¸åŒå®ä¾‹ä¸‹ä¹Ÿäº§ç”Ÿç›¸åŒçš„éšæœºæ•°ã€‚æ¯”å¦‚åœ¨ä¸Šé¢çš„è¯­å¥ä¸­ï¼Œ`test_size`ä¸º0.2ï¼Œå³é€‰æ‹©æ€»æ ·æœ¬çš„20%ä½œä¸ºæµ‹è¯•é›†ï¼Œä½†æ˜¯å¦‚ä½•é€‰æ‹©å‘¢ï¼Ÿ`random_state`å°±æŒ‡å®šäº†ï¼šæŒ‰ç…§â€œç¬¬42ç§â€è§„åˆ™é€‰æ‹©è¿™20%éšæœºçš„æ•°æ®ã€‚



## å®Œæ•´ä»£ç 

```python
"""
Author: Toky
Description: åŸºäºæœºå™¨å­¦ä¹ çš„æ¶æ„URLæ£€æµ‹
"""
import pickle

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression


def csv_data_read(csv_file_path):
    # ä¸ºå‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå¯åªå–å¤´éƒ¨10Wæ¡ï¼Œä½†ä¸€å®šéœ€è¦å…ˆæ‰“ä¹±æ ·æœ¬ï¼‰
    # df_csv = pd.read_csv(csv_file_path).head(100000)
    df_csv = pd.read_csv(csv_file_path)
    urls = []
    labels = []
    for index, row in df_csv.iterrows():
        urls.append(row["url"])
        labels.append(row["label"])
    return urls, labels


def url_tokenize(url):
    """
    å¯¹URLè¿›è¡Œæ¸…æ´—ï¼Œåˆ é™¤æ–œçº¿ã€ç‚¹ã€å’Œcomï¼Œè¿›è¡Œåˆ†è¯
    :param url:
    :return:
    """
    web_url = url.lower()
    dot_slash = []
    slash = str(web_url).split('/')
    for i in slash:
        r1 = str(i).split('-')
        token_slash = []
        for j in range(0,len(r1)):
            r2 = str(r1[j]).split('.')
            token_slash = token_slash + r2
        dot_slash = dot_slash + r1 + token_slash
    urltoken_list = list(set(dot_slash))
    white_words = ["com", "http:", "https:", ""]
    for white_word in white_words:
        if white_word in urltoken_list:
            urltoken_list.remove(white_word)
    return urltoken_list


def dump_model_object(file_path):
    """
    ä½¿ç”¨pickleå°†å†…å­˜ä¸­çš„å¯¹è±¡è½¬æ¢ä¸ºæ–‡æœ¬æµä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶
    :param file_path:
    :return:
    """
    with open(file_path, "wb") as f:
        pickle.dump(l_regress, f)
    f.close()


if __name__ == '__main__':
    """
    åŠ è½½æ•°æ®é›†
    """
    grep_csv_file_path = "../../data/data-0x3/grey-url.csv"
    black_csv_file_path = "../../data/data-0x3/black-url.csv"
    grey_urls, y = csv_data_read(grep_csv_file_path)

    """
    ä½¿ç”¨TF-IDFç®—æ³•æå–å…³é”®è¯ç‰¹å¾ï¼Œå¹¶å°†æ•°æ®å¸§åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    """
    url_vectorizer = TfidfVectorizer(tokenizer=url_tokenize)
    x = url_vectorizer.fit_transform(grey_urls)
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

    """
    å¯¹æ•°æ®å¸§æ‰§è¡Œé€»è¾‘å›å½’ï¼Œå°†æ‹Ÿåˆåçš„æ¨¡å‹å’Œå‘é‡ä¿å­˜
    """
    # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‰§è¡Œé€»è¾‘å›å½’
    l_regress = LogisticRegression(solver='liblinear')
    l_regress.fit(x_train, y_train)
    l_score = l_regress.score(x_test, y_test)
    print("æµ‹è¯•æ‹Ÿåˆåˆ†æ•°ä¸ºï¼š{0}".format(l_score))

    file_mode = "../../model/model-0x3/model.pkl"
    dump_model_object(file_mode)
    file_vector = "../../model/model-0x3/vector.pl"
    dump_model_object(file_vector)
```



## ToDo

